{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "894d082b-ef72-42fb-8758-c6f5e9db0d79",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bbc39f1-20d2-49eb-941d-230c0a5c9b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from utils.train import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1818473-969c-45eb-bf06-192fe52a43fe",
   "metadata": {},
   "source": [
    "# Specifying trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9857bf2-548d-4224-bbac-c4cb10a26c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = '3fcaf56e-b2bf-4d27-bd6d-896febd504e9'\n",
    "\n",
    "with open(f'experiments/{EXP_NAME}/config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "config = edict(config)\n",
    "config.SIZE = (241, 336, 283)\n",
    "\n",
    "os.makedirs(f'experiments/{EXP_NAME}/predictions_', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e32507-03c2-47ca-b959-ff2e18beb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_dict = {}\n",
    "for feature in config.FEATURES:\n",
    "    all_data_dict[feature] = [f\"{config.path_to_data}/{feature}/{subject}.nii\" for subject in config.subjects]\n",
    "all_data_dict['brains'] = [f\"{config.path_to_data}/{config.BRAIN_MODALITY}_brains/{subject}.nii\" for subject in config.subjects]\n",
    "all_data_dict['labels'] = [f\"{config.path_to_data}/labels/{subject}.nii\" for subject in config.subjects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06bfe672-01c9-4b15-8503-b356abe322e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "\n",
    "from utils.crop import BrainDataSegCrop\n",
    "from utils.model_etc import build_multi_part_segmentation\n",
    "\n",
    "def preprocess_data_for_model(point_cloud, mask_of_points_repetition):\n",
    "    \"\"\"\n",
    "    Function preprocess point cloud and mask of points repetion for model, also create feature tesnor from point cloud tensor\n",
    "    \n",
    "    :params point_cloud:\n",
    "    :params mask_of_points_repetition:\n",
    "    \n",
    "    :outputs point_cloud:\n",
    "    :outputs mask_of_points_repetition:\n",
    "    :outputs features:\n",
    "    \"\"\"\n",
    "    \n",
    "    point_cloud = point_cloud.unsqueeze(0)\n",
    "    mask_of_points_repetition = mask_of_points_repetition.unsqueeze(0)\n",
    "    features = point_cloud.transpose(1, 2).contiguous()\n",
    "    \n",
    "    features = features.cuda(non_blocking=True)\n",
    "    point_cloud = point_cloud[:, :, :3].cuda(non_blocking=True)\n",
    "    mask_of_points_repetition = mask_of_points_repetition.cuda(non_blocking=True)\n",
    "    \n",
    "    return point_cloud, mask_of_points_repetition, features\n",
    "\n",
    "def inference(all_data_dict, weights_path, config):\n",
    "    \"\"\"\n",
    "    Function \n",
    "    \n",
    "    :params\n",
    "    :outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    model, _ = build_multi_part_segmentation(config=config)\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "        \n",
    "    for idx in tqdm(range(len(all_data_dict['brains']))):\n",
    "        \n",
    "        subject = all_data_dict['brains'][idx].split('/')[-1].split('.')[0]\n",
    "        one_brain_dict = {key: all_data_dict[key][idx] for key in all_data_dict}\n",
    "        one_brain_crops_data_loader = BrainDataSegCrop(config=config, task='test', data_dict=one_brain_dict, return_air_mask_test=True)\n",
    "\n",
    "        original_point_cloud_flattened = []\n",
    "        labels_flattened = []\n",
    "        softmax_predictions_flattened = []\n",
    "        air_masks_flattened = []\n",
    "\n",
    "        for crop in one_brain_crops_data_loader:\n",
    "\n",
    "            original_point_cloud, mask_of_points_repetition, labels, air_mask = [crop[key] for key in [\"current_points\", \"mask\", \"current_points_labels\", \"air_mask\"]]\n",
    "\n",
    "            point_cloud, mask_of_points_repetition, features = preprocess_data_for_model(original_point_cloud, mask_of_points_repetition)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predictions = model(point_cloud, mask_of_points_repetition, features)\n",
    "\n",
    "            softmax_predictions_flattened += torch.softmax(predictions[0], dim=1)[0, 1, :].reshape(-1).detach().cpu().numpy().tolist()\n",
    "            labels_flattened += labels.reshape(-1).detach().cpu().numpy().tolist()\n",
    "            air_masks_flattened += air_mask.reshape(-1).tolist()\n",
    "\n",
    "            if config.IS_RETURN_ABS_COORDS:\n",
    "                means = np.array([x // 2 for x in config.SIZE])\n",
    "                half_range = np.array([x // 2 for x in config.SIZE])\n",
    "                original_point_cloud_flattened += (original_point_cloud[:, :3].detach().cpu().numpy() * half_range + means).tolist()\n",
    "            else:\n",
    "                original_point_cloud_flattened += (original_point_cloud[:, :3].detach().cpu().numpy()).tolist()\n",
    "\n",
    "\n",
    "        resulting_dict = {'coordinates': original_point_cloud_flattened,\n",
    "                          'predictions': softmax_predictions_flattened,\n",
    "                          'labels': labels_flattened,\n",
    "                          'air_maks': air_masks_flattened}\n",
    "        \n",
    "        with open(f'experiments/{config.EXP_NAME}/predictions_/{subject}.json', 'w') as file:\n",
    "                json.dump(resulting_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b424597-73bf-48fc-86cd-c02f4588fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 1\n",
    "torch.cuda.set_device(f\"cuda:{DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4d9a12-60ef-4a86-8e52-35f4db9ee969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [07:40<00:00, 24.25s/it]\n"
     ]
    }
   ],
   "source": [
    "weights_path = 'experiments/c3adc367-329a-4189-8fd0-69d97ec26f1f/weights/1_fold.pth'\n",
    "inference(all_data_dict=all_data_dict, weights_path=weights_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271338d5-5811-4199-82d1-a8e83fe401a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
